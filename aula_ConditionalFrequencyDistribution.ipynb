{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a68d888-7b95-4bda-bced-58f70c1c4519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('all')\n",
    "# nltk.download('gutenberg')\n",
    "# nltk.download('genesis')\n",
    "# nltk.download('inaugural')\n",
    "# nltk.download('nps_chat')\n",
    "# nltk.download('webtext')\n",
    "# nltk.download('treebank')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('omw')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('brown')\n",
    "\n",
    "from nltk.book import *\n",
    "\n",
    "#import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8100b844-faa0-42b7-95e9-db440dc87346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text1)\n",
    "n_words = len(text1)\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c0d083-c747-41de-8e86-6c893849da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenização: ['Este', 'é', 'um', 'exemplo', 'de', 'frase', 'para', 'tokenização', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenização\n",
    "text = \"Este é um exemplo de frase para tokenização.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Tokenização:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d96b3652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remoção de stopwords: ['exemplo', 'frase', 'tokenização', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remoção de stopwords\n",
    "stop_words = set(stopwords.words(\"portuguese\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(\"Remoção de stopwords:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35eb332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: run\n",
      "Lemmatização: running\n"
     ]
    }
   ],
   "source": [
    "# Stemming e lematização\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmed_word = stemmer.stem(\"running\")\n",
    "lemmatized_word = lemmatizer.lemmatize(\"running\")\n",
    "print(\"Stemming:\", stemmed_word)\n",
    "print(\"Lemmatização:\", lemmatized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d2477f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging: [('Este', 'NN'), ('é', 'NNP'), ('um', 'JJ'), ('exemplo', 'NN'), ('de', 'IN'), ('frase', 'NN'), ('para', 'NN'), ('tokenização', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-Speech (POS) tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS tagging:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa4695c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Análise sintática\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# %pip install spacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# !python -m spacy download pt_core_news_sm\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      8\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt_core_news_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA casa é bonita.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\FH3I\\PycharmProjects\\NLPPython\\venv\\lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\FH3I\\PycharmProjects\\NLPPython\\venv\\lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32mc:\\Users\\FH3I\\PycharmProjects\\NLPPython\\venv\\lib\\site-packages\\spacy\\compat.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[0;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[1;32mc:\\Users\\FH3I\\PycharmProjects\\NLPPython\\venv\\lib\\site-packages\\thinc\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CupyOps,\n\u001b[0;32m      3\u001b[0m     MPSOps,\n\u001b[0;32m      4\u001b[0m     NumpyOps,\n\u001b[0;32m      5\u001b[0m     Ops,\n\u001b[0;32m      6\u001b[0m     get_current_ops,\n\u001b[0;32m      7\u001b[0m     get_ops,\n\u001b[0;32m      8\u001b[0m     set_current_ops,\n\u001b[0;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[0;32m     10\u001b[0m     use_ops,\n\u001b[0;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[0;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[1;32mc:\\Users\\FH3I\\PycharmProjects\\NLPPython\\venv\\lib\\site-packages\\thinc\\backends\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[1;32mc:\\Users\\FH3I\\PycharmProjects\\NLPPython\\venv\\lib\\site-packages\\thinc\\backends\\cupy_ops.py:16\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     is_cupy_array,\n\u001b[0;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     torch2xp,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[1;32mc:\\Users\\FH3I\\PycharmProjects\\NLPPython\\venv\\lib\\site-packages\\thinc\\backends\\numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Análise sintática\n",
    "\n",
    "# %pip install spacy\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "sentence = \"A casa é bonita.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(\"Árvore de análise sintática:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.dep_} - {token.head.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50a58fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades nomeadas:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntidades nomeadas:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchunks\u001b[49m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(chunk, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mlabel(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunk))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Entidades nomeadas:\")\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk, \"label\"):\n",
    "        print(chunk.label(), \" \".join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de sentimentos\n",
    "text = \"Este filme é incrível!\"\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment = sia.polarity_scores(text)\n",
    "print(\"Análise de sentimentos:\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a269cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelagem de tópicos\n",
    "words = nltk.corpus.brown.words(categories=\"news\")\n",
    "fdist = nltk.FreqDist(words)\n",
    "top_words = fdist.most_common(10)\n",
    "print(\"Top palavras:\", top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificação de texto\n",
    "training_data = [(\"Este filme é ótimo\", \"positivo\"),\n",
    "                 (\"O filme é terrível\", \"negativo\")]\n",
    "features = [(nltk.word_tokenize(text), label) for (text, label) in training_data]\n",
    "classifier = nltk.NaiveBayesClassifier.train(features)\n",
    "text = \"Este filme é fantástico!\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "classification = classifier.classify(dict([(token, True) for token in tokens]))\n",
    "print(\"Classificação de texto:\", classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7fc0cc-357f-422e-92ba-68c36c15de0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'apple': 2, 'kiwi': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conditional Frequency\n",
    "items = ['apple', 'apple', 'kiwi', 'cabbage', 'cabbage', 'potato']\n",
    "nltk.FreqDist(items)\n",
    "FreqDist({'apple': 2, 'cabbage': 2, 'kiwi': 1, 'potato': 1})\n",
    "\n",
    "c_items = [('F','apple'), ('F','apple'), ('F','kiwi'), ('V','cabbage'), ('V','cabbage'), ('V','potato') ]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(c_items)\n",
    "cfd.conditions()\n",
    "['V', 'F']\n",
    "cfd['V']\n",
    "FreqDist({'cabbage': 2, 'potato': 1})\n",
    "cfd['F']\n",
    "FreqDist({'apple': 2, 'kiwi': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd6fd71-e9c2-4c7e-9d0a-1fc7bb0da785",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Counting Words by Genre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Now let's determine the frequency of words, of a particular genre, in brown corpus.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m cfd \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mConditionalFreqDist([ (genre, word) \u001b[38;5;28;01mfor\u001b[39;00m genre \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbrown\u001b[49m\u001b[38;5;241m.\u001b[39mcategories() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m brown\u001b[38;5;241m.\u001b[39mwords(categories\u001b[38;5;241m=\u001b[39mgenre) ])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# The conditions applied can be viewed as shown below.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m cfd\u001b[38;5;241m.\u001b[39mconditions()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'brown' is not defined"
     ]
    }
   ],
   "source": [
    "# Counting Words by Genre\n",
    "# Now let's determine the frequency of words, of a particular genre, in brown corpus.\n",
    "cfd = nltk.ConditionalFreqDist([ (genre, word) for genre in brown.categories() for word in brown.words(categories=genre) ])\n",
    "\n",
    "# The conditions applied can be viewed as shown below.\n",
    "cfd.conditions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91b612e5-39cc-450f-a3ce-0fb372b5f490",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Counting Words by Genre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Now let's determine the frequency of words, of a particular genre, in brown corpus.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m cfd \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mConditionalFreqDist([ \n\u001b[0;32m      4\u001b[0m (genre, word) \n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m genre \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbrown\u001b[49m\u001b[38;5;241m.\u001b[39mcategories() \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m brown\u001b[38;5;241m.\u001b[39mwords(categories\u001b[38;5;241m=\u001b[39mgenre) ])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# The conditions applied can be viewed as shown below.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m cfd\u001b[38;5;241m.\u001b[39mconditions()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'brown' is not defined"
     ]
    }
   ],
   "source": [
    "# Counting Words by Genre\n",
    "# Now let's determine the frequency of words, of a particular genre, in brown corpus.\n",
    "cfd = nltk.ConditionalFreqDist([ \n",
    "(genre, word) \n",
    "for genre in brown.categories() \n",
    "for word in brown.words(categories=genre) ])\n",
    "# The conditions applied can be viewed as shown below.\n",
    "cfd.conditions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94970a7a-84fe-40b6-8f53-f8f3e4df4cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condições: ['condicao1', 'condicao2', 'condicao3']\n",
      "Palavras para a condição1: dict_keys(['palavra1', 'palavra2', 'palavra3'])\n",
      "Número total de valores: 11\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConditionalFreqDist' object has no attribute 'B'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m total_values \u001b[38;5;241m=\u001b[39m cfdist\u001b[38;5;241m.\u001b[39mN()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNúmero total de valores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_values)\n\u001b[1;32m---> 25\u001b[0m total_conditions \u001b[38;5;241m=\u001b[39m \u001b[43mcfdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNúmero total de condições:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_conditions)\n\u001b[0;32m     28\u001b[0m most_common \u001b[38;5;241m=\u001b[39m cfdist\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConditionalFreqDist' object has no attribute 'B'"
     ]
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Criar uma distribuição de frequência condicional\n",
    "cfdist = ConditionalFreqDist()\n",
    "\n",
    "# Adicionar dados à distribuição de frequência condicional\n",
    "cfdist['condicao1'] = FreqDist(['palavra1', 'palavra1', 'palavra2', 'palavra3'])\n",
    "cfdist['condicao2'] = FreqDist(['palavra1', 'palavra2', 'palavra2', 'palavra3'])\n",
    "cfdist['condicao3'] = FreqDist(['palavra1', 'palavra1', 'palavra3'])\n",
    "\n",
    "# Utilizar os métodos da distribuição de frequência condicional\n",
    "conditions = cfdist.conditions()\n",
    "print(\"Condições:\", conditions)\n",
    "\n",
    "keys = cfdist['condicao1'].keys()\n",
    "print(\"Palavras para a condição1:\", keys)\n",
    "\n",
    "freqdist = cfdist.freqdist('condicao1')\n",
    "print(\"Distribuição de frequência para a condição1:\", freqdist)\n",
    "\n",
    "total_values = cfdist.N()\n",
    "print(\"Número total de valores:\", total_values)\n",
    "\n",
    "total_conditions = cfdist.B()\n",
    "print(\"Número total de condições:\", total_conditions)\n",
    "\n",
    "most_common = cfdist.most_common(3)\n",
    "print(\"Mais comuns:\", most_common)\n",
    "\n",
    "max_condition = cfdist.max()\n",
    "print(\"Condição com maior valor:\", max_condition)\n",
    "\n",
    "cfdist['condicao1'].tabulate()\n",
    "cfdist['condicao1'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32751b10-107d-4ff7-a3b7-b54315853d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           leadership    worship   hardship \n",
      "government          0          0          0 \n",
      "     humor          0          0          0 \n",
      "   reviews          0          0          0 \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c941f51a-97a4-495f-adac-d5e872f9068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           leadership    worship   hardship \n",
      "government          0          0          0 \n",
      "     humor          0          0          0 \n",
      "   reviews          0          0          0 \n"
     ]
    }
   ],
   "source": [
    "# Viewing Cumulative Word Count\n",
    "cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'], cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e48e5bc-495d-42fc-a854-9be998271a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 5580), (',', 5188), ('.', 4030)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing Individual Frequency Distributions\n",
    "news_fd = cfd['news']\n",
    "news_fd.most_common(3)\n",
    "# [('the', 5580), (',', 5188), ('.', 4030)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "440f70cf-9403-4c41-81b4-859247300f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can further access count of any sample as shown below.\n",
    "news_fd['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3aed5e6b-7eb3-4246-8be2-5c0cf774e247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing Frequency Distributions\n",
    "from nltk.corpus import names\n",
    "nt = [(fid.split('.')[0], name[-1])    for fid in names.fileids() \n",
    "  for name in names.words(fid) ]\n",
    "cfd2 = nltk.ConditionalFreqDist(nt)\n",
    "sum([cfd2['female'][x] for x in cfd2['female']]) > sum([cfd2['male'][x] for x in cfd2['male']])\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e440a6ab-dbf9-46ea-99f6-b3f66181cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a    e \n",
      "female 1773 1432 \n",
      "  male   29  468 \n"
     ]
    }
   ],
   "source": [
    "# Comparing Frequency Distributions\n",
    "# The following code snippet displays frequency count of characters a and e in females and males, respectively.\n",
    "cfd2.tabulate(samples=['a', 'e'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
